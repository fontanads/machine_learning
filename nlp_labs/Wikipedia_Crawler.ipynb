{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wikipedia\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scraper collects corpus from Wikipedia in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Wikipedia_Crawler` class uses the `wikipedia` module to load a _corpus_ in real time.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nlp_classes import Wikipedia_Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to set the desired language of the searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will search specific wikis from the input query and downloads its content.\n",
    "- `query_list` is the list of query text to look for in Wikipedia\n",
    "- `max_results` is the maximum number of pages to load per query expression in the `query_list`\n",
    "- `max_v` will be used later to limit the vocabulary size of our custom dictionary\n",
    "- `skip_top` is the number of most frequent words (descending order) to be discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list  = ['Rainha','Rei','Mulher','Homem'] # 'Brasil','Japão','Família','Tempo','Calendário'  \n",
    "max_results = 3\n",
    "max_V       = 2000\n",
    "skip_top    = 0\n",
    "max_len_sentence = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell, we instantiate the crawler with its basic query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_crawler = Wikipedia_Crawler(query_list=query_list,max_results_per_query=max_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the desired wiki pages, we call for `query_wiki`. <br>\n",
    "The methods `get_wiki_sentences` downloads and tokenizes the sentences of the pages. <br> \n",
    "Then, `get_all_tokens` lists all unique tokens (words) in the data. Tokenization is performed in the process. <br>\n",
    "Finally, `count_vocabulary` counts the absolute frequency of each token. Tokenization is performed in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_crawler.query_wiki()\n",
    "wiki_crawler.get_wiki_sentences()\n",
    "wiki_crawler.get_all_tokens()\n",
    "wiki_crawler.count_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the tokens may be prohibitive. Thus, it is necessary to limit the vocabulary. <br>\n",
    "- `generate_vocabulary` will create `word2idx` and `idx2word` dictionaries \n",
    "- - dictionaries will be limited with `max_V` words ($+ 3$ tags)\n",
    "- - note that `skip_top` most frequent words will be discarded\n",
    "- `encode_all_sentence` will encode sentences of words into sequences (lists) of corresponding indexes\n",
    "- - encoded sentences will have additional `\"<START>\"` and `\"<END>\"` tokens\n",
    "- - words out of vocabulary will be coded with index of `\"<OOV>\"` tag\n",
    "- - encoded senteces will be truncated with `max_len_sentence` actual words ($+ 2$ start/end tags)\n",
    "- - encoded senteces padded by default, unless `pad_sentences` is set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_crawler.generate_vocabulary(max_len_vocabulary=max_V, skip_top_words=skip_top)\n",
    "wiki_crawler.encode_all_sentence(max_len_sentence=max_len_sentence, pad_sentences=True) # pad_sentences is True by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check where did the corpus came from and other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corpus derived from the following wiki pages: \n",
      " ---- ['Rainha', 'A Rainha', 'Rainha Elizabeth', 'Rei', 'Rei (xadrez)', 'Choque Rei', 'Mulher', 'Mulher, Mulher', 'Mulher-Gato', 'Homem', 'Homem-Aranha', 'Homem-Aranha no cinema']\n",
      " Number of sentences: ------ 1301\n",
      " Max. words in sentence: --- 100\n",
      " Vocabulary Size: ---------- 2003\n"
     ]
    }
   ],
   "source": [
    "S = len(wiki_crawler.sentences)           \n",
    "T = len(wiki_crawler.coded_sentences[0]) # max_len_sentence + 2\n",
    "V = len(wiki_crawler.word2idx)           # max_V + 2\n",
    "\n",
    "print(f' Corpus derived from the following wiki pages: \\n ---- {wiki_crawler.titles_list}')\n",
    "print(f' Number of sentences: ------ {S}')\n",
    "print(f' Max. words in sentence: --- {T-2}')\n",
    "print(f' Vocabulary Size: ---------- {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can check the generated sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Real Text sentence: \n",
      "    entre os reis davi de judá e israel, não é mencionado uma única rainha reinante; apesar de atália, embora a bíblia se refira negativamente como uma usurpadora.\n",
      "\n",
      " Encoded Sentence: \n",
      "    [1, 49, 17, 543, 1392, 3, 1934, 6, 1935, 26, 23, 1936, 12, 375, 40, 195, 153, 3, 1937, 186, 4, 221, 20, 1938, 1393, 15, 12, 1939, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " Decoded Sentence: \n",
      "    <START> entre os reis davi de judá e israel não é mencionado uma única rainha reinante apesar de atália embora a bíblia se refira negativamente como uma usurpadora <END>\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 5\n",
    "\n",
    "real_sentence    = wiki_crawler.sentences[num_sentence]\n",
    "encoded_sentece  = wiki_crawler.coded_sentences[num_sentence]\n",
    "decoded_sentence = wiki_crawler.decode_one_sentence(encoded_sentece)\n",
    "\n",
    "print(f' Real Text sentence: \\n    {real_sentence}', end=2*'\\n')\n",
    "print(f' Encoded Sentence: \\n    {encoded_sentece}', end=2*'\\n')\n",
    "print(f' Decoded Sentence: \\n    '+' '.join(decoded_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word \"mulher\" corresponds to index 36\n",
      " Index 9 corresponds to word \"do\"\n"
     ]
    }
   ],
   "source": [
    "word = 'mulher'\n",
    "idx = 9\n",
    "print(f' Word \"{word}\" corresponds to index {wiki_crawler.word2idx[word]}')\n",
    "print(f' Index {idx} corresponds to word \"{wiki_crawler.idx2word[idx]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
